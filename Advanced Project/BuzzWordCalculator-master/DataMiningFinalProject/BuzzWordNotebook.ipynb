{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation, metrics\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "import pickle\n",
    "import gevent.monkey\n",
    "import re\n",
    "import html\n",
    "from collections import Counter\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "#import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/OnlineNewsPopularity.csv\")\n",
    "\n",
    "\n",
    "urls = data[\"url\"]\n",
    "shares = data[\" shares\"]\n",
    "titles = []\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon's Streaming Video Library Now a Little Easier to Navigate 0\n",
      "Celebrities Read Mean Tweets About Themselves (Round 3) 100\n",
      "Digg Talks Monetization, Unveils New Ad Product   200\n",
      "Startup Wants to Use Drones to Deliver Medical Supplies 300\n"
     ]
    }
   ],
   "source": [
    "#WEBSITE SCRAPER\n",
    "i = 0\n",
    "for url in urls:\n",
    "    try:\n",
    "        #print('Starting {}'.format(url))\n",
    "        req = Request(url)\n",
    "        response = urlopen(req)\n",
    "        page = str(response.read())\n",
    "        #print(page)\n",
    "        match = re.search('<title>(.*?)</title>', page)\n",
    "        title = match.group(1) if match else \"ERROR\"\n",
    "        fixedTitle = html.unescape(title)\n",
    "        #print(fixedTitle)\n",
    "    #        titleFix1 = str(title).replace(\"<title>\", \"\")\n",
    "    #        titleFix2 = str(titleFix1).replace(\"</title>\", \"\")\n",
    "    #        print(titleFix2)\n",
    "        #titles[numPlace] = fixedTitle\n",
    "        if(i%500 == 0):\n",
    "            print(fixedTitle, i)\n",
    "        i = i + 1\n",
    "        #print(numPlace, \"DONE\")\n",
    "        titles.append(fixedTitle)\n",
    "    except:\n",
    "        titles.append(\"Error\")\n",
    "        \n",
    "print(\"ALL DONE\")\n",
    "\n",
    "\n",
    "print(\"TITLES\", titles)\n",
    "print(len(titles))\n",
    "\n",
    "with open(\"titles\", 'wb') as f:\n",
    "    pickle.dump(titles, f)\n",
    "####################WEB SCRAPE DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b39e92272a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##READS TITLES FROM PREPROCESSED TITLES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titles\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mreadTitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titles'"
     ]
    }
   ],
   "source": [
    "##READS TITLES FROM PREPROCESSED TITLES\n",
    "with open(\"titles\", 'rb') as f:\n",
    "    readTitles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################Organize Weight of Words in terms of shares and sort the dictionary    \n",
    "titleDict = {}\n",
    "k = 0\n",
    "for title in readTitles:\n",
    "    titleDict[title] = shares[k]\n",
    "    k = k + 1\n",
    "\n",
    "finalDict = {}\n",
    "for title in titleDict:\n",
    "    split = title.split()\n",
    "    for word in split:\n",
    "        finalDict[word] = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "\n",
    "i = 0\n",
    "for title in titleDict:\n",
    "    wordDict = title.split()\n",
    "    for word in wordDict:\n",
    "        finalDict[word][0] = finalDict[word][0] + 1\n",
    "        finalDict[word][1] = finalDict[word][1] + titleDict[title]\n",
    "        if data[' data_channel_is_lifestyle'][i] == 1:\n",
    "            finalDict[word][2] = finalDict[word][2] + titleDict[title]\n",
    "        if data[' data_channel_is_entertainment'][i] == 1:\n",
    "            finalDict[word][3] = finalDict[word][3] + titleDict[title]\n",
    "        if data[' data_channel_is_bus'][i] == 1:\n",
    "            finalDict[word][4] = finalDict[word][4] + titleDict[title]\n",
    "        if data[' data_channel_is_socmed'][i] == 1:\n",
    "            finalDict[word][5] = finalDict[word][5] + titleDict[title]\n",
    "        if data[' data_channel_is_tech'][i] == 1:\n",
    "            finalDict[word][6] = finalDict[word][6] + titleDict[title]\n",
    "        if data[' data_channel_is_world'][i] == 1:\n",
    "            finalDict[word][7] = finalDict[word][7] + titleDict[title]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

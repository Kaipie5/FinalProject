{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation, metrics\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "import pickle\n",
    "import re\n",
    "import html\n",
    "from collections import Counter\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "#Grabbing data downloaded from UC Irvine's dataset repository\n",
    "data = pd.read_csv(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/OnlineNewsPopularity.csv\")\n",
    "\n",
    "\n",
    "urls = data[\"url\"]\n",
    "shares = data[\" shares\"]\n",
    "titles = []\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon's Streaming Video Library Now a Little Easier to Navigate 0\n",
      "This Is What Little Mark Zuckerberg Looked Like [PIC] 500\n",
      "iRobot's Medical Robot Gets FDA Approval for Hospital Use 1000\n",
      "Facebook Campaign Rescues 'Gay Bulldog' From Execution 1500\n",
      "The Worst Type of YouTube User [SUNDAY COMICS] 2000\n",
      "Google Music Streaming Rumors and More News You Need to Know 2500\n",
      "Jeopardy Burns Taylor Swift [PIC]  3000\n",
      "10 of the Funniest Beer Commercials in the World 3500\n",
      "High-Speed Police Chase Caught on Helmet Cam 4000\n",
      "15 Adorable Things That Delight Lil Bub [PICS] 4500\n",
      "Space Station Crew Returns to Earth Monday: How to Watch Live 5000\n",
      "Bystander Live-Tweets Brutal London Machete Attack 5500\n",
      "Why Would-be Marijuana Moguls Have Their Work Cut Out For Them 6000\n",
      "12 Sketchbooks of the Pros [COMIC] 6500\n",
      "15 Dead Things on Etsy to Give Your Friends 7000\n"
     ]
    }
   ],
   "source": [
    "#WEBSITE SCRAPER\n",
    "#Scrapes titles from each URL in the datset WARNING: Takes hours to run\n",
    "i = 0\n",
    "for url in urls:\n",
    "    try:\n",
    "        req = Request(url)\n",
    "        response = urlopen(req)\n",
    "        page = str(response.read())\n",
    "        match = re.search('<title>(.*?)</title>', page)\n",
    "        title = match.group(1) if match else \"ERROR\"\n",
    "        fixedTitle = html.unescape(title)\n",
    "        if(i%1000 == 0):\n",
    "            print(fixedTitle, i)\n",
    "        i = i + 1\n",
    "        titles.append(fixedTitle)\n",
    "    except:\n",
    "        titles.append(\"Error\")\n",
    "        \n",
    "print(\"ALL DONE\")\n",
    "\n",
    "\n",
    "print(\"TITLES\", titles)\n",
    "print(len(titles))\n",
    "\n",
    "with open(\"titles\", 'wb') as f:\n",
    "    pickle.dump(titles, f)\n",
    "####################WEB SCRAPE DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "##READS TITLES FROM PREPROCESSED TITLES\n",
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/titles\", 'rb') as f:\n",
    "    readTitles = pickle.load(f)\n",
    "print(\"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################Organize Weight of Words in terms of shares and sort the dictionary    \n",
    "titleDict = {}\n",
    "k = 0\n",
    "for title in readTitles:\n",
    "    titleDict[title] = shares[k]\n",
    "    k = k + 1\n",
    "\n",
    "finalDict = {}\n",
    "for title in titleDict:\n",
    "    split = title.split()\n",
    "    for word in split:\n",
    "        finalDict[word] = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "\n",
    "i = 0\n",
    "for title in titleDict:\n",
    "    wordDict = title.split()\n",
    "    for word in wordDict:\n",
    "        finalDict[word][0] = finalDict[word][0] + 1\n",
    "        finalDict[word][1] = finalDict[word][1] + titleDict[title]\n",
    "        if data[' data_channel_is_lifestyle'][i] == 1:\n",
    "            finalDict[word][2] = finalDict[word][2] + titleDict[title]\n",
    "        if data[' data_channel_is_entertainment'][i] == 1:\n",
    "            finalDict[word][3] = finalDict[word][3] + titleDict[title]\n",
    "        if data[' data_channel_is_bus'][i] == 1:\n",
    "            finalDict[word][4] = finalDict[word][4] + titleDict[title]\n",
    "        if data[' data_channel_is_socmed'][i] == 1:\n",
    "            finalDict[word][5] = finalDict[word][5] + titleDict[title]\n",
    "        if data[' data_channel_is_tech'][i] == 1:\n",
    "            finalDict[word][6] = finalDict[word][6] + titleDict[title]\n",
    "        if data[' data_channel_is_world'][i] == 1:\n",
    "            finalDict[word][7] = finalDict[word][7] + titleDict[title]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(index):\n",
    "    fixedDict = {}\n",
    "    for val in finalDict:\n",
    "        fixedDict[val] = finalDict[val][index]/finalDict[val][0]\n",
    "    #Sort\n",
    "    fixedDict = sorted(fixedDict.items(), key=operator.itemgetter(1))\n",
    "    fixedDict.reverse()\n",
    "    #Analysis\n",
    "    indexString = \"Index\" + str(index)\n",
    "    with open(str(indexString), 'wb') as f:\n",
    "        pickle.dump(fixedDict, f)\n",
    "\n",
    "for x in range(1, 8):\n",
    "    setup(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  all\n",
      "('880', 441000.0)\n",
      "('Inequality', 309500.0)\n",
      "('Sucking', 221250.0)\n",
      "('Lectures', 220166.66666666666)\n",
      "('Leaked:', 212108.5)\n",
      "('believing', 211600.0)\n",
      "('myths', 211600.0)\n",
      "('Extent', 207066.66666666666)\n",
      "('suspension', 200100.0)\n",
      "('overturns', 200100.0)\n",
      "('Advised', 193400.0)\n",
      "('McDonalds', 193400.0)\n",
      "('Wearably', 180600.0)\n",
      "('Ocho', 161600.0)\n",
      "(\"Age,'\", 161600.0)\n",
      "('Aviary,', 161600.0)\n",
      "('Wealth', 155223.25)\n",
      "('hires', 142198.8)\n",
      "('Teenagers', 139600.0)\n",
      "('$32.5', 139500.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  lifestyle\n",
      "('200-Mile', 111300.0)\n",
      "('revenues', 69000.0)\n",
      "('POWs:', 57500.0)\n",
      "('Commandos', 57500.0)\n",
      "('Awakens', 50900.0)\n",
      "(\"'Almost\", 50900.0)\n",
      "('tops', 50100.0)\n",
      "('active', 50100.0)\n",
      "('users,', 50100.0)\n",
      "('Coca-Cola?', 47800.0)\n",
      "('Tables,', 42500.0)\n",
      "('Outgrown', 40000.0)\n",
      "('All-Muppet', 36200.0)\n",
      "('Out?', 36100.0)\n",
      "('Alibaba?', 34800.0)\n",
      "('bigger', 33400.0)\n",
      "('likely', 33400.0)\n",
      "('Pigeons', 28750.0)\n",
      "('Crashed?', 28300.0)\n",
      "('Flees', 26550.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  entertainment\n",
      "('Inequality', 308950.0)\n",
      "('believing', 211600.0)\n",
      "('myths', 211600.0)\n",
      "('Extent', 206733.33333333334)\n",
      "('suspension', 200100.0)\n",
      "('overturns', 200100.0)\n",
      "('Wealth', 154475.0)\n",
      "('Frustrating', 112600.0)\n",
      "(\"Tutor'\", 112600.0)\n",
      "('MapBox', 106400.0)\n",
      "(\"'Donate\", 104600.0)\n",
      "('Inspiration,', 96900.0)\n",
      "('Oaida:', 96900.0)\n",
      "('judge', 66913.66666666667)\n",
      "('Filterstorm', 65900.0)\n",
      "('Tracker,', 65900.0)\n",
      "('Striiv', 65900.0)\n",
      "('SoundHound,', 65900.0)\n",
      "('Free:', 65900.0)\n",
      "('Today\\\\xe2\\\\x80\\\\x99s', 65900.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  business\n",
      "('Wearably', 180600.0)\n",
      "('7.0.4', 128500.0)\n",
      "('Halen', 97200.0)\n",
      "('Luma', 96100.0)\n",
      "('53%', 95100.0)\n",
      "('Sext,', 80800.0)\n",
      "('Copied', 72450.0)\n",
      "('Exits', 70600.0)\n",
      "('Winningest', 70600.0)\n",
      "('Periods', 70200.0)\n",
      "(\"'Forgotten'\", 69500.0)\n",
      "('Memberships', 69300.0)\n",
      "('Thrilled', 59400.0)\n",
      "('Fan-Created', 57100.0)\n",
      "('Lookout', 54550.0)\n",
      "(\"'Alarmed'\", 53800.0)\n",
      "(\"Address'\", 53500.0)\n",
      "(\"'Gettysburg\", 53500.0)\n",
      "('Politicians,', 53500.0)\n",
      "('hired', 51900.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  medical\n",
      "('Advised', 193400.0)\n",
      "('McDonalds', 193400.0)\n",
      "('$32.5', 139500.0)\n",
      "('NetWars', 54900.0)\n",
      "('Performing', 51400.0)\n",
      "('Payroll', 50100.0)\n",
      "('NimbleTV', 44700.0)\n",
      "('Seattle:', 41100.0)\n",
      "('GTA', 38000.0)\n",
      "('Accidental', 34875.0)\n",
      "('Meals', 32450.0)\n",
      "('fifth', 32100.0)\n",
      "('Smooths', 29700.0)\n",
      "('tense', 29200.0)\n",
      "('marches', 29200.0)\n",
      "(\"'Selma'\", 29200.0)\n",
      "('in,', 29200.0)\n",
      "('1-0', 27700.0)\n",
      "(\"Vortex'\", 27700.0)\n",
      "('Telef\\\\xc3\\\\xb3nica', 27300.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  tech\n",
      "('880', 441000.0)\n",
      "('Sucking', 220500.0)\n",
      "('Lectures', 219466.66666666666)\n",
      "('Leaked:', 210825.0)\n",
      "('Ocho', 161600.0)\n",
      "(\"Age,'\", 161600.0)\n",
      "('Aviary,', 161600.0)\n",
      "(\"Tom's\", 113650.0)\n",
      "('Obstacles', 111450.0)\n",
      "('Power,', 110250.0)\n",
      "('Inroads', 98000.0)\n",
      "('Electro-Pop', 92600.0)\n",
      "('Phantogram:', 92600.0)\n",
      "('Realizations', 87000.0)\n",
      "('Low-Cost', 84771.5)\n",
      "('Corridor', 77600.0)\n",
      "('Dove', 76711.11111111111)\n",
      "('Roomba', 74750.0)\n",
      "('Ceres', 74300.0)\n",
      "('Harvard', 72544.44444444444)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************  world\n",
      "('hires', 141540.0)\n",
      "('Teenagers', 139600.0)\n",
      "('Sprout-Powered', 122800.0)\n",
      "('DB10', 119100.0)\n",
      "(\"Bond's\", 119100.0)\n",
      "('Parodied:', 112500.0)\n",
      "('Influencing', 110200.0)\n",
      "('Guarantee', 105549.0)\n",
      "('Frustratedly', 104100.0)\n",
      "('Masses', 99466.66666666667)\n",
      "('morale', 87600.0)\n",
      "('Africa\\\\xe2\\\\x80\\\\x99s', 84800.0)\n",
      "('Over.', 79900.0)\n",
      "('Ant-Man', 77400.0)\n",
      "('act', 75500.0)\n",
      "('entering', 75500.0)\n",
      "('\"Nature\"', 71300.0)\n",
      "('Detox?', 71300.0)\n",
      "('shutterbug', 67100.0)\n",
      "('Infomercial', 65300.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo = [\"\", \"all\", \"lifestyle\", \"entertainment\", \"business\", \"medical\", \"tech\", \"world\"]\n",
    "analysisDict = {}\n",
    "def analyze(index):\n",
    "    indexString = \"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/Index\" + str(index)\n",
    "    with open(indexString, 'rb') as f:\n",
    "        analysisList = pickle.load(f)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"******************* \", foo[index])\n",
    "    for i in range(0, 20):\n",
    "        print(analysisList[i])\n",
    "    plotList = []\n",
    "    for i in range(0, 800):\n",
    "        plotList.append(analysisList[i][1])\n",
    "    plt.plot(plotList)\n",
    "    analysisDict[index] = analysisList\n",
    "    \n",
    "for g in range(1, 8):\n",
    "    analyze(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-16391066f4ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mnewColumnSetUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/FULLCSV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-16391066f4ef>\u001b[0m in \u001b[0;36mnewColumnSetUp\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mcalculatedVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mindexOfWord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcalculatedVal\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0manalysisDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexOfWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxShareWord\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtitleShares\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxShareWord\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mcalculatedVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysisDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexOfWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxShareWord\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtitleShares\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxShareWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Sum of frequency of all words in title \n",
    "\n",
    "def newColumnSetUp(index):\n",
    "    #Title: A B C\n",
    "    #F(A) = shares of A\n",
    "    #Max = top word in All\n",
    "    #MAX(F(A)/F(MAX), F(B)/F(MAX), F(C)/F(MAX))  \n",
    "    #Do this for each channell\n",
    "    #Put all of these in as new columns in my data\n",
    "    with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/titles\", 'rb') as f:\n",
    "        readTitles = pickle.load(f)\n",
    "        \n",
    "    indexString = \"Index\" + str(index)\n",
    "    with open(indexString, 'rb') as f:\n",
    "        analysisDict = pickle.load(f)\n",
    "    #First value is largest share word. (Dictionary was sorted earlier)\n",
    "    maxShareWord = analysisDict[0][1]\n",
    "    newColumn = []\n",
    "    searchDict = []\n",
    "    for i in range(0, len(analysisDict)):\n",
    "        searchDict.append(analysisDict[i][0])\n",
    "    k = 0\n",
    "    for title in readTitles:\n",
    "        if index == 1 or (index == 2 and data[' data_channel_is_lifestyle'][k]) or (index == 3 and data[' data_channel_is_entertainment'][k]) or (index == 4 and data[' data_channel_is_bus'][k]) or (index == 5 and data[' data_channel_is_socmed'][k]) or (index == 6 and data[' data_channel_is_tech'][k]) or (index == 7 and data[' data_channel_is_world'][k]):\n",
    "            titleShares = data[' shares'][k]\n",
    "            split = title.split()\n",
    "            calculatedVal = 0.0\n",
    "            for word in split:\n",
    "                indexOfWord = searchDict.index(word)\n",
    "                if calculatedVal < analysisDict[indexOfWord][1]/maxShareWord - titleShares/maxShareWord:\n",
    "                    calculatedVal = analysisDict[indexOfWord][1]/maxShareWord - titleShares/maxShareWord\n",
    "            newColumn.append(calculatedVal) \n",
    "        else:\n",
    "            newColumn.append(0)\n",
    "        k = k + 1\n",
    "    columnName = foo[index] + \" word prediction\"\n",
    "    data[columnName] = newColumn\n",
    "    \n",
    "\n",
    "\n",
    "for g in range(1,8):\n",
    "    newColumnSetUp(g)\n",
    "    \n",
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/FULLCSV\", 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/FULLCSV\", 'rb') as f:\n",
    "    newData = pickle.load(f)\n",
    "print(\"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysisDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-65b4fef84aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalysisDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mwordLengthAnalysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordLengthAnalysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analysisDict' is not defined"
     ]
    }
   ],
   "source": [
    "#########Size Of Word Analysis##########\n",
    "\n",
    "\n",
    "wordLengthAnalysis = []\n",
    "for v in range(0, 40):\n",
    "    wordLengthAnalysis.append(0)\n",
    "\n",
    "for i in range(1, 8):\n",
    "    for word in analysisDict[i]:\n",
    "        wordLengthAnalysis[len(word[0])] = wordLengthAnalysis[len(word[0])] + word[1]\n",
    "\n",
    "print(wordLengthAnalysis)\n",
    "    \n",
    "removedOutliers = []\n",
    "for i in range(0, len(wordLengthAnalysis)):\n",
    "    if wordLengthAnalysis[i] > 1200000:\n",
    "        removedOutliers.append(wordLengthAnalysis[i])\n",
    "        \n",
    "print(removedOutliers)\n",
    "print(\"WORD COUNT ANALYSIS\")\n",
    "plt.plot(removedOutliers)\n",
    "\n",
    "\n",
    "\n",
    "#########Size Of Word Analysis FINISHED#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree on Original Data Set\n",
      "Acurracy:  0.5832895428271152\n",
      "RandomForest on Original Data Set\n",
      "Acurracy:  0.6634892275354703\n",
      "NaiveBayes on Original Data Set\n",
      "Acurracy:  0.6083026799789807\n",
      "DONE WITH ORIGINAL DATA\n"
     ]
    }
   ],
   "source": [
    "########MODEL TESTING###########\n",
    "\n",
    "popular = data[' shares'] >= 1400\n",
    "unpopular = data[' shares'] < 1400\n",
    "\n",
    "data.loc[popular,' shares'] = 1\n",
    "data.loc[unpopular,' shares'] = 0\n",
    "score_dt = 0\n",
    "score_rf = 0\n",
    "score_nb = 0\n",
    "numRuns = 10\n",
    "for i in range(0, numRuns):\n",
    "    data.sample(frac=1)\n",
    "    #split original dataset into 60% training and 40% testing\n",
    "    features=list(data.columns[2:60])\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(data[features], data[' shares'], test_size=0.4, random_state=0)\n",
    "\n",
    "    x_rest, x_test_part, y_rest, y_test_part= cross_validation.train_test_split(x_test, y_test,\n",
    "                                                                                test_size=0.6, random_state=0)\n",
    "    \n",
    "    decisionTree = DecisionTreeClassifier(min_samples_split=20,random_state=99)\n",
    "    clf_dt=decisionTree.fit(x_train,y_train)\n",
    "    newScore = clf_dt.score(x_test_part,y_test_part)\n",
    "    score_dt=score_dt + newScore\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    clf_rf = rf.fit(x_train,y_train)\n",
    "    newScore = clf_rf.score(x_test_part,y_test_part)\n",
    "    score_rf=score_rf + newScore\n",
    "    \n",
    "\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    clf_nb=nb.fit(x_train,y_train)\n",
    "    newScore = clf_nb.score(x_test_part,y_test_part)\n",
    "    score_nb=score_nb + newScore\n",
    "\n",
    "score_dt = score_dt/numRuns\n",
    "score_rf = score_rf/numRuns\n",
    "score_nb = score_nb/numRuns\n",
    "print(\"DecisionTree on Original Data Set\")\n",
    "print(\"Acurracy: \", score_dt)\n",
    "print(\"RandomForest on Original Data Set\")\n",
    "print(\"Acurracy: \", score_rf)\n",
    "print(\"NaiveBayes on Original Data Set\")\n",
    "print(\"Acurracy: \", score_nb)\n",
    "\n",
    "print(\"DONE WITH ORIGINAL DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree on New Data Set\n",
      "Acurracy:  0.733893851812927\n",
      "RandomForest on New Data Set\n",
      "Acurracy:  0.7851287440882817\n",
      "NaiveBayes on New Data Set\n",
      "Acurracy:  0.702679978980557\n",
      "DONE WITH NEW DATA\n"
     ]
    }
   ],
   "source": [
    "########MODEL TESTING###########\n",
    "\n",
    "popular = newData[' shares'] >= 1400\n",
    "unpopular = newData[' shares'] < 1400\n",
    "\n",
    "newData.loc[popular,' shares'] = 1\n",
    "newData.loc[unpopular,' shares'] = 0\n",
    "indices = []\n",
    "for i in range(2,68):\n",
    "    if i != 60:\n",
    "        indices.append(i)\n",
    "        \n",
    "score_dt = 0\n",
    "score_rf = 0\n",
    "score_nb = 0\n",
    "numRuns = 10\n",
    "for i in range(0, numRuns):\n",
    "    newData.sample(frac=1)\n",
    "    #split original dataset into 60% training and 40% testing\n",
    "    features=list(newData.columns[indices])\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(newData[features], newData[' shares'], test_size=0.4, random_state=0)\n",
    "\n",
    "    x_rest, x_test_part, y_rest, y_test_part= cross_validation.train_test_split(x_test, y_test,\n",
    "                                                                                test_size=0.6, random_state=0)\n",
    "    \n",
    "    decisionTree = DecisionTreeClassifier(min_samples_split=20,random_state=99)\n",
    "    clf_dt=decisionTree.fit(x_train,y_train)\n",
    "    newScore = clf_dt.score(x_test_part,y_test_part)\n",
    "    score_dt=score_dt + newScore\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    clf_rf = rf.fit(x_train,y_train)\n",
    "    newScore = clf_rf.score(x_test_part,y_test_part)\n",
    "    score_rf=score_rf + newScore\n",
    "    \n",
    "\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    clf_nb=nb.fit(x_train,y_train)\n",
    "    newScore = clf_nb.score(x_test_part,y_test_part)\n",
    "    score_nb=score_nb + newScore\n",
    "\n",
    "score_dt = score_dt/numRuns\n",
    "score_rf = score_rf/numRuns\n",
    "score_nb = score_nb/numRuns\n",
    "print(\"DecisionTree on New Data Set\")\n",
    "print(\"Acurracy: \", score_dt)\n",
    "print(\"RandomForest on New Data Set\")\n",
    "print(\"Acurracy: \", score_rf)\n",
    "print(\"NaiveBayes on New Data Set\")\n",
    "print(\"Acurracy: \", score_nb)\n",
    "\n",
    "print(\"DONE WITH NEW DATA\")\n",
    "##########MODEL TESTING FINISHED############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/FULLCSV\", 'rb') as f:\n",
    "    newData = pickle.load(f)\n",
    "print(\"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part of Speech\n",
    "##New feature for each part of speech and boolean of whether it has that part of speech or not.\n",
    "import nltk as nltk\n",
    "coordinatingConjunctions = []\n",
    "cardinalDigits = []\n",
    "determiners = []\n",
    "existentialTheres = []\n",
    "foreignWords = []\n",
    "prepositions = []\n",
    "bigAdjectives = []\n",
    "biggerAdjectives = []\n",
    "biggestAdjectives = []\n",
    "listMarkers = []\n",
    "modals = []\n",
    "singleNouns = []\n",
    "pluralNouns = []\n",
    "properNouns = []\n",
    "pluralProperNouns = []\n",
    "predeterminers = []\n",
    "possesives = []\n",
    "personalPronouns = []\n",
    "possesivePronouns = []\n",
    "adverbs = []\n",
    "comparitiveAdverbs = []\n",
    "superlativeAdverbs = []\n",
    "particles = []\n",
    "toBe = []\n",
    "interjections = []\n",
    "verbs = []\n",
    "gerundVerbs = []\n",
    "pastVerbs = []\n",
    "pastParticipleVerbs = []\n",
    "presentVerbs = []\n",
    "verbs3rdPerson = []\n",
    "whDeterminers = []\n",
    "whPronouns = []\n",
    "whPossesives = []\n",
    "whAdverbs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39644\n"
     ]
    }
   ],
   "source": [
    "for title in readTitles:\n",
    "    tokenized = nltk.word_tokenize(title)\n",
    "    partsOfSpeech = nltk.pos_tag(tokenized)\n",
    "    tokens = []\n",
    "    for token in partsOfSpeech:\n",
    "        tokens.append(token[1])\n",
    "    if \"CC\" in tokens:\n",
    "        coordinatingConjunctions.append(1)\n",
    "    else:\n",
    "        coordinatingConjunctions.append(0)\n",
    "    if \"CD\" in tokens:\n",
    "        cardinalDigits.append(1)\n",
    "    else:\n",
    "        cardinalDigits.append(0)\n",
    "    if \"DT\" in tokens:\n",
    "        determiners.append(1)\n",
    "    else:\n",
    "        determiners.append(0)\n",
    "    if \"EX\" in tokens:\n",
    "        existentialTheres.append(1)\n",
    "    else:\n",
    "        existentialTheres.append(0)\n",
    "    if \"FW\" in tokens:\n",
    "        foreignWords.append(1)\n",
    "    else:\n",
    "        foreignWords.append(0)\n",
    "    if \"IN\" in tokens:\n",
    "        prepositions.append(1)\n",
    "    else:\n",
    "        prepositions.append(0)\n",
    "    if \"JJ\" in tokens:\n",
    "        bigAdjectives.append(1)\n",
    "    else:\n",
    "        bigAdjectives.append(0)\n",
    "    if \"JJR\" in tokens:\n",
    "        biggerAdjectives.append(1)\n",
    "    else:\n",
    "        biggerAdjectives.append(0)\n",
    "    if \"JJS\" in tokens:\n",
    "        biggestAdjectives.append(1)\n",
    "    else:\n",
    "        biggestAdjectives.append(0)\n",
    "    if \"LS\" in tokens:\n",
    "        listMarkers.append(1)\n",
    "    else:\n",
    "        listMarkers.append(0)\n",
    "    if \"MD\" in tokens:\n",
    "        modals.append(1)\n",
    "    else:\n",
    "        modals.append(0)\n",
    "    if \"NN\" in tokens:\n",
    "        singleNouns.append(1)\n",
    "    else:\n",
    "        singleNouns.append(0)\n",
    "    if \"NNS\" in tokens:\n",
    "        pluralNouns.append(1)\n",
    "    else:\n",
    "        pluralNouns.append(0)\n",
    "    if \"NNP\" in tokens:\n",
    "        properNouns.append(1)\n",
    "    else:\n",
    "        properNouns.append(0)\n",
    "    if \"NNPS\" in tokens:\n",
    "        pluralProperNouns.append(1)\n",
    "    else:\n",
    "        pluralProperNouns.append(0)\n",
    "    if \"PDT\" in tokens:\n",
    "        predeterminers.append(1)\n",
    "    else:\n",
    "        predeterminers.append(0)\n",
    "    if \"POS\" in tokens:\n",
    "        possesives.append(1)\n",
    "    else:\n",
    "        possesives.append(0)\n",
    "    if \"PRP\" in tokens:\n",
    "        personalPronouns.append(1)\n",
    "    else:\n",
    "        personalPronouns.append(0)\n",
    "    if \"PRP$\" in tokens:\n",
    "        possesivePronouns.append(1)\n",
    "    else:\n",
    "        possesivePronouns.append(0)\n",
    "    if \"RB\" in tokens:\n",
    "        adverbs.append(1)\n",
    "    else:\n",
    "        adverbs.append(0)\n",
    "    if \"RBR\" in tokens:\n",
    "        comparitiveAdverbs.append(1)\n",
    "    else:\n",
    "        comparitiveAdverbs.append(0)\n",
    "    if \"RBS\" in tokens:\n",
    "        superlativeAdverbs.append(1)\n",
    "    else:\n",
    "        superlativeAdverbs.append(0)\n",
    "    if \"RP\" in tokens:\n",
    "        particles.append(1)\n",
    "    else:\n",
    "        particles.append(0)\n",
    "    if \"TO\" in tokens:\n",
    "        toBe.append(1)\n",
    "    else:\n",
    "        toBe.append(0)\n",
    "    if \"UH\" in tokens:\n",
    "        interjections.append(1)\n",
    "    else:\n",
    "        interjections.append(0)\n",
    "    if \"VB\" in tokens:\n",
    "        verbs.append(1)\n",
    "    else:\n",
    "        verbs.append(0)\n",
    "    if \"VBD\" in tokens:\n",
    "        pastVerbs.append(1)\n",
    "    else:\n",
    "        pastVerbs.append(0)\n",
    "    if \"VBG\" in tokens:\n",
    "        gerundVerbs.append(1)\n",
    "    else:\n",
    "        gerundVerbs.append(0)\n",
    "    if \"VBN\" in tokens:\n",
    "        pastParticipleVerbs.append(1)\n",
    "    else:\n",
    "        pastParticipleVerbs.append(0)\n",
    "    if \"VBP\" in tokens:\n",
    "        presentVerbs.append(1)\n",
    "    else:\n",
    "        presentVerbs.append(0)\n",
    "    if \"VBZ\" in tokens:\n",
    "        verbs3rdPerson.append(1)\n",
    "    else:\n",
    "        verbs3rdPerson.append(0)\n",
    "    if \"WDT\" in tokens:\n",
    "        whDeterminers.append(1)\n",
    "    else:\n",
    "        whDeterminers.append(0)\n",
    "    if \"WP\" in tokens:\n",
    "        whPronouns.append(1)\n",
    "    else:\n",
    "        whPronouns.append(0)\n",
    "    if \"WP$\" in tokens:\n",
    "        whPossesives.append(1)\n",
    "    else:\n",
    "        whPossesives.append(0)\n",
    "    if \"WRB\" in tokens:\n",
    "        whAdverbs.append(1)\n",
    "    else:\n",
    "        whAdverbs.append(0)\n",
    "print(len(whAdverbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = \"coordinatingConjunctions\"\n",
    "newData[columnName] = coordinatingConjunctions\n",
    "columnName = \"cardinalDigits\"\n",
    "newData[columnName] = cardinalDigits\n",
    "columnName = \"determiners\"\n",
    "newData[columnName] = determiners\n",
    "columnName = \"existentialTheres\"\n",
    "newData[columnName] = existentialTheres\n",
    "columnName = \"foreignWords\"\n",
    "newData[columnName] = foreignWords\n",
    "columnName = \"prepositions\"\n",
    "newData[columnName] = prepositions\n",
    "columnName = \"bigAdjectives\"\n",
    "newData[columnName] = bigAdjectives\n",
    "columnName = \"biggerAdjectives\"\n",
    "newData[columnName] = biggerAdjectives\n",
    "columnName = \"biggestAdjectives\"\n",
    "newData[columnName] = biggestAdjectives\n",
    "columnName = \"listMarkers\"\n",
    "newData[columnName] = listMarkers\n",
    "columnName = \"modals\"\n",
    "newData[columnName] = modals\n",
    "columnName = \"singleNouns\"\n",
    "newData[columnName] = singleNouns\n",
    "columnName = \"pluralNouns\"\n",
    "newData[columnName] = pluralNouns\n",
    "columnName = \"properNouns\"\n",
    "newData[columnName] = properNouns\n",
    "columnName = \"pluralProperNouns\"\n",
    "newData[columnName] = pluralProperNouns\n",
    "columnName = \"predeterminers\"\n",
    "newData[columnName] = predeterminers\n",
    "columnName = \"possesives\"\n",
    "newData[columnName] = possesives\n",
    "columnName = \"personalPronouns\"\n",
    "newData[columnName] = personalPronouns\n",
    "columnName = \"possesivePronouns\"\n",
    "newData[columnName] = possesivePronouns\n",
    "columnName = \"adverbs\"\n",
    "newData[columnName] = adverbs\n",
    "columnName = \"comparitiveAdverbs\"\n",
    "newData[columnName] = comparitiveAdverbs\n",
    "columnName = \"superlativeAdverbs\"\n",
    "newData[columnName] = superlativeAdverbs\n",
    "columnName = \"particles\"\n",
    "newData[columnName] = particles\n",
    "columnName = \"toBe\"\n",
    "newData[columnName] = toBe\n",
    "columnName = \"interjections\"\n",
    "newData[columnName] = interjections\n",
    "columnName = \"verbs\"\n",
    "newData[columnName] = verbs\n",
    "columnName = \"gerundVerbs\"\n",
    "newData[columnName] = gerundVerbs\n",
    "columnName = \"pastVerbs\"\n",
    "newData[columnName] = pastVerbs\n",
    "columnName = \"presentVerbs\"\n",
    "newData[columnName] = presentVerbs\n",
    "columnName = \"verbs3rdPerson\"\n",
    "newData[columnName] = verbs3rdPerson\n",
    "columnName = \"whDeterminers\"\n",
    "newData[columnName] = whDeterminers\n",
    "columnName = \"whPronouns\"\n",
    "newData[columnName] = whPronouns\n",
    "columnName = \"whPossesives\"\n",
    "newData[columnName] = whPossesives\n",
    "columnName = \"whAdverbs\"\n",
    "newData[columnName] = whAdverbs\n",
    "##Is title a question?####\n",
    "newColumn = []\n",
    "for title in readTitles:\n",
    "    if '?' in title:\n",
    "        newColumn.append(1)\n",
    "    else:\n",
    "        newColumn.append(0)\n",
    "columnName = \"IsItAQuestion?\"\n",
    "newData[columnName] = newColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "DecisionTree on Parts Of Speech Data Set\n",
      "Acurracy:  0.7484237074401009\n",
      "RandomForest on Parts of Speech Data Set\n",
      "Acurracy:  0.7988335435056746\n",
      "NaiveBayes on Parts of Speech Data Set\n",
      "Acurracy:  0.712484237074401\n",
      "DONE WITH Parts Of Speech DATA\n"
     ]
    }
   ],
   "source": [
    "########MODEL TESTING###########\n",
    "\n",
    "popular = newData[' shares'] >= 1400\n",
    "unpopular = newData[' shares'] < 1400\n",
    "\n",
    "newData.loc[popular,' shares'] = 1\n",
    "newData.loc[unpopular,' shares'] = 0\n",
    "print(len(newData.columns))\n",
    "for i in range(2,103):\n",
    "    if i != 60:\n",
    "        indices.append(i)\n",
    "        \n",
    "score_dt = 0\n",
    "score_rf = 0\n",
    "score_nb = 0\n",
    "numRuns = 10\n",
    "for i in range(0, numRuns):\n",
    "    newData.sample(frac=1)\n",
    "    #split original dataset into 60% training and 40% testing\n",
    "    features=list(newData.columns[indices])\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(newData[features], newData[' shares'], test_size=0.2, random_state=0)\n",
    "\n",
    "    x_rest, x_test_part, y_rest, y_test_part= cross_validation.train_test_split(x_test, y_test,\n",
    "                                                                                test_size=0.8, random_state=0)\n",
    "    \n",
    "    decisionTree = DecisionTreeClassifier(min_samples_split=20,random_state=99)\n",
    "    clf_dt=decisionTree.fit(x_train,y_train)\n",
    "    newScore = clf_dt.score(x_test_part,y_test_part)\n",
    "    score_dt=score_dt + newScore\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    clf_rf = rf.fit(x_train,y_train)\n",
    "    newScore = clf_rf.score(x_test_part,y_test_part)\n",
    "    score_rf=score_rf + newScore\n",
    "    \n",
    "\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    clf_nb=nb.fit(x_train,y_train)\n",
    "    newScore = clf_nb.score(x_test_part,y_test_part)\n",
    "    score_nb=score_nb + newScore\n",
    "\n",
    "score_dt = score_dt/numRuns\n",
    "score_rf = score_rf/numRuns\n",
    "score_nb = score_nb/numRuns\n",
    "print(\"DecisionTree on Parts Of Speech Data Set\")\n",
    "print(\"Acurracy: \", score_dt)\n",
    "print(\"RandomForest on Parts of Speech Data Set\")\n",
    "print(\"Acurracy: \", score_rf)\n",
    "print(\"NaiveBayes on Parts of Speech Data Set\")\n",
    "print(\"Acurracy: \", score_nb)\n",
    "\n",
    "print(\"DONE WITH Parts Of Speech DATA\")\n",
    "##########MODEL TESTING FINISHED############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/FULLCSV\", 'rb') as f:\n",
    "    newData = pickle.load(f)\n",
    "print(\"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Amazon's\", 'Streaming', 'Video', 'Library', 'Now', 'a', 'Little', 'Easier', 'to', 'Navigate']\n",
      "39644\n"
     ]
    }
   ],
   "source": [
    "##Word Embeddings################\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "sentences = []\n",
    "for title in readTitles:\n",
    "    newSentence = []\n",
    "    for word in title.split():\n",
    "        newSentence.append(word)\n",
    "    sentences.append(newSentence)\n",
    "\n",
    "print(sentences[0])\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=44258, size=100, alpha=0.025)\n",
      "[ 0.34276783  0.46069226 -0.16278662 -0.16949683  0.22781396  0.61216336\n",
      " -0.09008636  0.04603339  0.17321911 -0.09875596 -0.23456736 -0.17230874\n",
      "  0.3110582  -0.3687816   0.12025033  0.0223255   0.03771744  0.24812336\n",
      " -0.09969793 -0.35040304 -0.6068585  -0.06685096  0.0788237   0.39716873\n",
      "  0.08661266  0.4342817   0.21720888  0.2909831   0.30601093 -0.44079024\n",
      "  0.38317132  0.09469972  0.07694181 -0.45059064  0.3726229   0.11208489\n",
      "  0.4350454   0.09683706 -0.3045633   0.27395815  0.31626284  0.2387609\n",
      "  0.01654872 -0.12610699 -0.25478745 -0.08348373 -0.32510683 -0.452502\n",
      "  0.2597069  -0.37114    -0.06528524 -0.0764733   0.04796081 -0.04082907\n",
      " -0.06830652 -0.48895198 -0.09660547  0.02120616  0.13412653  0.3704928\n",
      " -0.44822082  0.17172104 -0.29963487 -0.3548699  -0.11822192  0.34526187\n",
      " -0.12054364  0.09974688 -0.2351686  -0.47744402  0.32261613  0.21066917\n",
      " -0.12642547  0.16821691 -0.45494872  0.21937929 -0.05528766  0.58340436\n",
      " -0.33688602 -0.31701666  0.29808363 -0.17980458  0.05236186 -0.73990226\n",
      " -0.5982032   0.11635125  0.08112606 -0.08322529  0.32400873  0.28626087\n",
      " -0.10933226 -0.13807695 -0.0628428   0.14118141  0.0483771   0.27331063\n",
      " -0.24048927 -0.13675214 -0.20551202 -0.45076725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "# print(words)\n",
    "print(model['Streaming'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/FULLCSV\", 'rb') as f:\n",
    "    newData = pickle.load(f)\n",
    "print(\"LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE MAKING VECTORS\n",
      "0       -0.893495\n",
      "1       -0.761654\n",
      "2       -0.738610\n",
      "3       -0.791363\n",
      "4       -1.198846\n",
      "5       -0.938808\n",
      "6       -0.671144\n",
      "7       -1.094782\n",
      "8       -0.889075\n",
      "9       -0.462816\n",
      "10      -0.942319\n",
      "11      -0.811234\n",
      "12      -0.861432\n",
      "13      -0.746236\n",
      "14      -0.587047\n",
      "15      -0.735157\n",
      "16      -1.098821\n",
      "17      -1.270627\n",
      "18      -1.411164\n",
      "19      -0.930544\n",
      "20      -1.098097\n",
      "21      -0.535460\n",
      "22      -0.616810\n",
      "23      -0.674164\n",
      "24      -0.597284\n",
      "25      -0.547977\n",
      "26      -0.459846\n",
      "27      -1.003859\n",
      "28      -1.229597\n",
      "29      -1.140541\n",
      "           ...   \n",
      "39614   -0.880668\n",
      "39615   -0.877733\n",
      "39616   -0.590425\n",
      "39617   -0.634233\n",
      "39618   -0.170875\n",
      "39619   -0.517786\n",
      "39620   -0.516453\n",
      "39621   -0.317207\n",
      "39622   -0.843900\n",
      "39623   -1.067456\n",
      "39624   -0.277822\n",
      "39625   -0.527363\n",
      "39626   -0.705442\n",
      "39627   -0.296848\n",
      "39628   -0.673005\n",
      "39629   -0.433405\n",
      "39630   -0.516598\n",
      "39631   -0.523961\n",
      "39632   -0.488026\n",
      "39633   -0.637971\n",
      "39634   -0.748643\n",
      "39635   -0.652857\n",
      "39636   -0.559535\n",
      "39637   -0.520845\n",
      "39638   -0.543845\n",
      "39639   -0.476562\n",
      "39640   -0.664480\n",
      "39641   -0.564688\n",
      "39642   -0.801023\n",
      "39643   -1.076244\n",
      "Name: WordEmbeddings35, Length: 39644, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#sum all vectors of words in each title divide by number of words take all values in vector and add each to new feature\n",
    "#adding vectors is simple addition and divison\n",
    "newVectors = []\n",
    "for title in readTitles:\n",
    "    newVector = []\n",
    "    for i in range(0,100):\n",
    "        sum = 0;\n",
    "        for word in title.split():\n",
    "            sum = sum + model[word][i]\n",
    "        newVector.append(sum/len(title.split()))\n",
    "    newVectors.append(newVector)\n",
    "print(\"DONE MAKING VECTORS\")\n",
    "\n",
    "for colIndex in range(0,100):\n",
    "    newColumn = []\n",
    "    columnName = \"WordEmbeddings\"+str(colIndex)\n",
    "    for vector in newVectors: \n",
    "        newColumn.append(vector[i])\n",
    "    newData[columnName] = newColumn\n",
    "    \n",
    "print(newData[\"WordEmbeddings35\"])\n",
    "with open(\"/Users/kaimcconnell/Whitman Spring 2019/Advanced Project/BuzzWordCalculator-master/DataMiningFinalProject/WORDEMBEDDINGSCSV\", 'wb') as f:\n",
    "        pickle.dump(newData, f)\n",
    "# print(data[columnName][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree on Word Embeddings Data Set\n",
      "Acurracy:  0.7411727616645649\n",
      "RandomForest on Word Embeddings Data Set\n",
      "Acurracy:  0.7970523329129886\n",
      "NaiveBayes on Word Embeddings Data Set\n",
      "Acurracy:  0.711065573770492\n",
      "DONE WITH Word Embeddings DATA\n"
     ]
    }
   ],
   "source": [
    "########MODEL TESTING###########\n",
    "\n",
    "popular = newData[' shares'] >= 1400\n",
    "unpopular = newData[' shares'] < 1400\n",
    "\n",
    "newData.loc[popular,' shares'] = 1\n",
    "newData.loc[unpopular,' shares'] = 0\n",
    "for i in range(2,len(newData.columns)):\n",
    "    if i != 60:\n",
    "        indices.append(i)\n",
    "score_dt = 0\n",
    "score_rf = 0\n",
    "score_nb = 0\n",
    "numRuns = 10\n",
    "for i in range(0, numRuns):\n",
    "    newData.sample(frac=1)\n",
    "    #split original dataset into 60% training and 40% testing\n",
    "    features=list(newData.columns[indices])\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(newData[features], newData[' shares'], test_size=0.2, random_state=0)\n",
    "\n",
    "    x_rest, x_test_part, y_rest, y_test_part= cross_validation.train_test_split(x_test, y_test,\n",
    "                                                                                test_size=0.8, random_state=0)\n",
    "    \n",
    "    decisionTree = DecisionTreeClassifier(min_samples_split=20,random_state=99)\n",
    "    clf_dt=decisionTree.fit(x_train,y_train)\n",
    "    newScore = clf_dt.score(x_test_part,y_test_part)\n",
    "    score_dt=score_dt + newScore\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    clf_rf = rf.fit(x_train,y_train)\n",
    "    newScore = clf_rf.score(x_test_part,y_test_part)\n",
    "    score_rf=score_rf + newScore\n",
    "    \n",
    "\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    clf_nb=nb.fit(x_train,y_train)\n",
    "    newScore = clf_nb.score(x_test_part,y_test_part)\n",
    "    score_nb=score_nb + newScore\n",
    "\n",
    "score_dt = score_dt/numRuns\n",
    "score_rf = score_rf/numRuns\n",
    "score_nb = score_nb/numRuns\n",
    "print(\"DecisionTree on Word Embeddings Data Set\")\n",
    "print(\"Acurracy: \", score_dt)\n",
    "print(\"RandomForest on Word Embeddings Data Set\")\n",
    "print(\"Acurracy: \", score_rf)\n",
    "print(\"NaiveBayes on Word Embeddings Data Set\")\n",
    "print(\"Acurracy: \", score_nb)\n",
    "\n",
    "print(\"DONE WITH Word Embeddings DATA\")\n",
    "##########MODEL TESTING FINISHED############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
